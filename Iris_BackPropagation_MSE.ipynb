{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DPEE1085_Iris_BackPropagation_MSE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateusokp/Deep-Learning/blob/main/Iris_BackPropagation_MSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_ItvbmClh7X"
      },
      "source": [
        "# Ponto de Partida para o Exercício F07\n",
        "\n",
        "Este código implementa uma rede neural feedforward, estilo perceptron, incluindo o algoritmo de treinamento _backpropagation_. Neste código, todas as camadas dessa rede neural, incluindo a última, são ativadas pela função de ativação sigmoide, sem nenhuma operação adicional à saída. O erro calculado é o erro quadrático médio (MSE) e o algoritmo backpropagation é implementado calculando as derivadas baseadas nesse erro.\n",
        "\n",
        "Apesar de servir para fins didáticos, em problemas práticos de classificação, onde as saídas desejadas estão no formato _one-hot_, geralmente empregamos saídas _softmax_ e função entropia cruzada como custo. Para mais detalhes, [veja este material](https://colab.research.google.com/drive/1nbuWr-9ALwMSbCAJe4U4VBH526P-B98L?usp=sharing).\n",
        "\n",
        "Sua tarefa é:\n",
        "* Fazer uma cópia deste código\n",
        "* Alterar o método `forward()` para que seja empregada a função softmax na camada de saída (apenas).\n",
        "* Alterar a função `train()` para que o erro e os deltas sejam calculados empregando a função de custo entropia cruzada (CE).\n",
        "* Executar o código do início ao fim, mostrando o custo diminuindo\n",
        "* Mostrar o resultado comparando a saída da rede neural com a saída desejada, para os dados de teste.\n",
        "\n",
        "Este código deverá ser compartilhado [através do formulário de entrega da tarefa](https://docs.google.com/forms/d/e/1FAIpQLSc1_kUBIi0Y1uBwC_p9YA7pcAHB6q9Z_QlpaYzQ6O4Y4iDLQQ/viewform?usp=sf_link), onde você deverá responder algumas perguntas de auto-avaliação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SxyZu2W3CUE"
      },
      "source": [
        "# Código\n",
        "\n",
        "Aqui baixamos o arquivo `Iris.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I9mhp-y2M1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18544169-3376-4084-ccee-51f024b7290f"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1d3NbjXro_BfnYpFm66ETBfe7ubAZPAoL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1d3NbjXro_BfnYpFm66ETBfe7ubAZPAoL\n",
            "To: /content/Iris.csv\n",
            "\r  0% 0.00/5.11k [00:00<?, ?B/s]\r100% 5.11k/5.11k [00:00<00:00, 4.37MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7WVykQC3Mdp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvTqZKzDnJln"
      },
      "source": [
        "# A variável f é o arquivo de nome Iris.csv\n",
        "f = open('Iris.csv','r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLJhW3bl3x6b"
      },
      "source": [
        "Nas linhas abaixo zeramos o cursor do arquivo e lemos todas as linhas do arquivo em `lines`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-KRHha1kLC"
      },
      "source": [
        "# A variável lines é uma lista que contém as strings\n",
        "# que representam cada linha do arquivo lido.\n",
        "lines = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLJHSh2i4MUf"
      },
      "source": [
        "Abaixo criamos as matrizes dos dados de entrada e saída correspondentes, vazias mas já com as dimensões corretas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2aAziOo7kA-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Aqui criamos as matrizes de entradas e saídas correspondentes\n",
        "# ainda vazias\n",
        "X = np.zeros((len(lines)-1,4)) # 4 entradas\n",
        "Y = np.zeros((len(lines)-1,3)) # 3 saídas (one-hot encoding)\n",
        "\n",
        "# Teremos 3 categorias. O vetor abaixo lista as strings\n",
        "# que representam as categorias possíveis\n",
        "cat = np.array(['Iris-setosa','Iris-versicolor','Iris-virginica'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqtS_fdF4mMR"
      },
      "source": [
        "No laço abaixo populamos as matrizes `X` e `Y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvBA3O9116lg"
      },
      "source": [
        "# Para cada linha do arquivo, exceto\n",
        "# a primeira linha que é o cabeçalho\n",
        "for i, line in enumerate(lines[1:]):\n",
        "\n",
        "  # Aqui decodificamos a linha para transformar\n",
        "  # de binário para caracteres ascii, e descartamos\n",
        "  # o último caractere que representa uma nova linha\n",
        "  s = line[:-1]\n",
        "\n",
        "  # Aqui separamos os dados por vírgulas,\n",
        "  # descartando o primeiro valor que é o id\n",
        "  # pois usaremos i do laço como id.\n",
        "  _,sl,sw,pl,pw,sp = s.split(',')\n",
        "\n",
        "  # Transformamos as strings que representam\n",
        "  # as dimensões de sépala e pétala para ponto\n",
        "  # flutuante.\n",
        "  sl = float(sl)\n",
        "  sw = float(sw)\n",
        "  pl = float(pl)\n",
        "  pw = float(pw)\n",
        "  \n",
        "  # Aqui populamos as matrizes X e Y com os dados\n",
        "  # coletados.\n",
        "  X[i:] = np.array([sl,sw,pl,pw])\n",
        "  Y[i:] = (cat == sp).astype('float') # Atenção para essa linha!\n",
        "\n",
        "  # A última linha acima merece uma explicação mais longa:\n",
        "  # Nessa linha fazemos um teste booleano, comparando cada\n",
        "  # elemento do vetor cat com a string daquela linha do arquivo.\n",
        "  # Para os elementos onde a comparação der verdadeiro, teremos\n",
        "  # um booleano True, e para os elementos onde a comparação\n",
        "  # der falso, teremos um booleano False. O resultado da comparação\n",
        "  # do vetor cat com a string sp, na expressão cat == sp resulta\n",
        "  # numa array booleana com mesmo formato que cat, mas com\n",
        "  # valores booleanos True ou False em cada posição. O método\n",
        "  # .astype('float') transforma o resultado em um array de float,\n",
        "  # com 1.0 representando True e 0.0 representando False.\n",
        "  # Então transformamos uma string como 'Iris-setosa' no vetor\n",
        "  # [1.0, 0.0, 0.0] e uma string como 'Iris-versicolor' no vetor\n",
        "  # [0.0, 1.0, 0.0] e assim por diante."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqlSmWlv6iVH"
      },
      "source": [
        "Nas linhas abaixo embaralhamos as amostras para não causar nenhum tipo de tendência no treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi-Xaru-YO1a"
      },
      "source": [
        "import random\n",
        "\n",
        "# Aqui criamos uma lista de índices\n",
        "# embaralhados\n",
        "indexes = list(range(150))\n",
        "random.shuffle(indexes)\n",
        "\n",
        "# Essa variável T indica quantas amostras\n",
        "# serão usadas para treinamento. As demais\n",
        "# serão usadas para validação\n",
        "T = 140\n",
        "\n",
        "# Aqui preparamos as matrizes dos pares\n",
        "# de dados de treinamento e validação.\n",
        "Xt = np.zeros((T,4))\n",
        "Yt = np.zeros((T,3))\n",
        "Xv = np.zeros((150-T,4))\n",
        "Yv = np.zeros((150-T,3))\n",
        "\n",
        "# Aqui preenchemos as matrizes com os\n",
        "# respectivos valores\n",
        "for i in range(0,T):\n",
        "  Xt[i,:] = X[indexes[i],:]\n",
        "  Yt[i,:] = Y[indexes[i],:]\n",
        "for i in range(0,150-T):\n",
        "  Xv[i,:] = X[indexes[T+i],:]\n",
        "  Yv[i,:] = Y[indexes[T+i],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD-W-rnS-s9W"
      },
      "source": [
        "O código abaixo implementa a rede neural com o método de backpropagation.\n",
        "\n",
        "É na célula abaixo que você fará suas alterações para solucionar o exercício F07."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXes0y1dcXTa"
      },
      "source": [
        "# Essa será a função de ativação\n",
        "# utilizada\n",
        "def softmax(x):\n",
        "  ex = np.exp(x)\n",
        "  s = np.sum(ex)\n",
        "  return ex/s\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "# O código abaixo implementa uma rede\n",
        "# neural ariticial estilo perceptron\n",
        "# com 4 entradas, 8 neurônios escondidos\n",
        "# e 3 saídas\n",
        "class Perceptron:\n",
        "  def __init__(self):\n",
        "\n",
        "    # Pesos e biases da entrada para a camada\n",
        "    # escondida\n",
        "    self.Wh = np.random.random((8,4))*2.0 - 1.0\n",
        "    self.bh = np.random.random((8,1))*2.0 - 1.0\n",
        "\n",
        "    # Pesos e bieases da camada escondida para\n",
        "    # a saída\n",
        "    self.Wo = np.random.random((3,8))*2.0 - 1.0\n",
        "    self.bo = np.random.random((3,1))*2.0 - 1.0\n",
        "\n",
        "    # Esse será o passo de aprendizagem\n",
        "    self.eta = 0.01\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # Essa função faz o cálculo da saída\n",
        "    # da rede neural no sentido direto.\n",
        "\n",
        "    # Essa linha garante que x tenha tamanho\n",
        "    # de 4 linhas e 1 coluna\n",
        "    x = np.reshape(x,(4,1))\n",
        "\n",
        "    # Calcula a soma ponderada para a camada escondida\n",
        "    # somado ao bias\n",
        "    self.sh = np.dot(self.Wh,x) + self.bh\n",
        "\n",
        "    # Função de ativação é aplicada à camada escondida\n",
        "    self.zh = sigmoid(self.sh)\n",
        "\n",
        "    # Calcula a soma ponderada para a camada de saída\n",
        "    # somado ao bias\n",
        "    self.so = np.dot(self.Wo,self.zh) + self.bo\n",
        "\n",
        "    # Função de ativação é aplciada à camada de saída\n",
        "    self.zo = softmax(self.so)\n",
        "    return self.zo\n",
        "\n",
        "  def train(self, X, Y):\n",
        "    # Essa função faz um ciclo de\n",
        "    # treinamento em todos os dados\n",
        "    # dos pares X e Y\n",
        "\n",
        "    # A variável Err é o erro acumulado\n",
        "    # que só serve para avaliar a qualidade\n",
        "    # final da rede neural. Na prática não\n",
        "    # precisamos dele apra o treinamento\n",
        "    Err = 0.0\n",
        "\n",
        "    # Na linha abaixo obtemos a quantidade\n",
        "    # total de pares\n",
        "    total = X.shape[0]\n",
        "\n",
        "    # Laço de treinamento conforme\n",
        "    # o erro de cada par\n",
        "    for i in range(total):\n",
        "\n",
        "      # Capturamos o vetor de entradas do par\n",
        "      x = X[i,:]\n",
        "      x = np.reshape(x,(4,1))\n",
        "\n",
        "      # Capturamos o vetor de saída do par\n",
        "      y_hat = Y[i,:]\n",
        "      y_hat = np.reshape(y_hat,(3,1))\n",
        "\n",
        "      # Fazemos o cálculo da saída da rede\n",
        "      # neural\n",
        "      y = self.forward(x)\n",
        "\n",
        "      # Calculamos o erro médio, para avaliar\n",
        "      # a evolução da performance. Isso não\n",
        "      # é usado para calcular o ajuste dos\n",
        "      # pesos e biases.\n",
        "      err = -np.sum(y_hat * np.log(y),axis=0)\n",
        "      Err = Err + err\n",
        "\n",
        "      # Aqui calculamos os deltas de trás para frente\n",
        "      # no sentido inverso (daí o nome backpropagation)\n",
        "\n",
        "      # Primeiro calculamos o delta do erro da saída\n",
        "      # Aqui multiplicamos o erro pela derivada\n",
        "      # da função de ativação na saída. A função\n",
        "      # sigmoide possui uma derivada interessante.\n",
        "      # se z é a sigmoide, a derivada é z*(1-z)\n",
        "      self.do = y - y_hat\n",
        "\n",
        "      # O delta da camada escondida é calculado\n",
        "      # usando os pesos para propagar o delta do erro\n",
        "      # da saída para o delta do erro da camada escondida\n",
        "      self.dh = np.dot(self.Wo.T, self.do) \\\n",
        "                 * self.zh * (1.0 - self.zh)\n",
        "\n",
        "      # Agora é só dar um passo, de tamanho eta, na\n",
        "      # direção oposta do gradiente. Lembrando que\n",
        "      # para os pesos, a derivada pessoal é gerada\n",
        "      # computando a multiplicação do delta do erro\n",
        "      # pela saída da camada anterior, gerando uma\n",
        "      # matriz resultante da multiplicação desses\n",
        "      # dois vetores. Para o bias basta usar o próprio\n",
        "      # delta do erro.\n",
        "      self.Wo = self.Wo - self.eta * np.dot(self.do,self.zh.T)\n",
        "      self.bo = self.bo - self.eta * self.do\n",
        "      self.Wh = self.Wh - self.eta * np.dot(self.dh,x.T)\n",
        "      self.bh = self.bh - self.eta * self.dh\n",
        "    \n",
        "    # Já fora do laço, dividimos o erro pelo total\n",
        "    # de amostras para normalizar\n",
        "    Err = Err / total\n",
        "\n",
        "    # Retornamos o erro\n",
        "    return Err"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de8U56yo-_TD"
      },
      "source": [
        "Abaixo executamos o código, treinando a rede neural com os dados de treinamento separados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOyjnTNkjbRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d583b113-f55c-4b5a-b6a8-9e81e170bd0d"
      },
      "source": [
        "# Criamos p que é nossa rede neural\n",
        "p = Perceptron()\n",
        "\n",
        "# Treinaremos 10 mil + 1 épocas\n",
        "for i in range(10001):\n",
        "  \n",
        "  # Aqui um passo de treinamento\n",
        "  Err = p.train(Xt,Yt)\n",
        "\n",
        "  # A cada mil passos mostramos o erro\n",
        "  # total para verificar se está mesmo\n",
        "  # diminuindo.\n",
        "  if not (i % 1000) or i == 0:\n",
        "    print('Err = ',Err)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Err =  [+1.16 ]\n",
            "Err =  [+0.07 ]\n",
            "Err =  [+0.06 ]\n",
            "Err =  [+0.05 ]\n",
            "Err =  [+0.05 ]\n",
            "Err =  [+0.04 ]\n",
            "Err =  [+0.04 ]\n",
            "Err =  [+0.04 ]\n",
            "Err =  [+0.03 ]\n",
            "Err =  [+0.03 ]\n",
            "Err =  [+0.03 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egEWbfBa_Nyw"
      },
      "source": [
        "Aqui vamos avaliar a performance dessa rede neural treinada, vendo como ela se sai nos dados separados para validação (dados para os quais ela nunca foi treinada)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CavDK5osyEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f0bd07-b90e-42f8-83c2-3935289f1f3c"
      },
      "source": [
        "# Essa linha abaixo serve para suprimir a notação\n",
        "# científica padrão do numpy para facilitar nosso\n",
        "# olho humano comparar os resultados\n",
        "np.set_printoptions(formatter={'float':lambda x: '%+01.2f ' % x})\n",
        "\n",
        "# Para cada amostra de treinamento\n",
        "# nesse laço\n",
        "for i in range(150-T):\n",
        "\n",
        "  # Separamos a entrada\n",
        "  xv = Xv[i,:]\n",
        "\n",
        "  # Calculamos a saída da rede\n",
        "  y = p.forward(xv)\n",
        "\n",
        "  # Separamos a saída esperada\n",
        "  yv = Yv[i,:]\n",
        "\n",
        "  # Mostramos ambos lado a lado\n",
        "  print(y.T[0], yv)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+1.00  +0.00  +0.00 ] [+1.00  +0.00  +0.00 ]\n",
            "[+0.00  +1.00  +0.00 ] [+0.00  +1.00  +0.00 ]\n",
            "[+0.00  +1.00  +0.00 ] [+0.00  +1.00  +0.00 ]\n",
            "[+0.00  +0.00  +1.00 ] [+0.00  +0.00  +1.00 ]\n",
            "[+0.00  +0.00  +1.00 ] [+0.00  +0.00  +1.00 ]\n",
            "[+0.00  +0.00  +1.00 ] [+0.00  +0.00  +1.00 ]\n",
            "[+0.00  +0.01  +0.99 ] [+0.00  +0.00  +1.00 ]\n",
            "[+0.00  +0.34  +0.66 ] [+0.00  +0.00  +1.00 ]\n",
            "[+0.00  +0.99  +0.01 ] [+0.00  +1.00  +0.00 ]\n",
            "[+0.00  +1.00  +0.00 ] [+0.00  +1.00  +0.00 ]\n"
          ]
        }
      ]
    }
  ]
}